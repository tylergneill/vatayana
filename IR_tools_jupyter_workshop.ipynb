{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create virtual server here in notebook with access to all functions and variables\n",
    "from IR_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search settings\n",
    "\n",
    "# PVA selection\n",
    "priority_texts = [\n",
    "                    \"VS\", \"MīmS\", \"MMK\", \"ViVy\", \"NS\", \"YSBh\", \"SK\", \n",
    "                    \"ViṃśV\", \"NBh\", \"MīmBh\", \"ĀP\", \"PSV\", \"NPS\", \"TriṃśBh\", \n",
    "                    \"YD\", \"PDhS\", \"NV\", \"PPad\", \"ŚV\", \"PVSV\", \"PV\", \"PVin\", \n",
    "                    \"HB\", \"NB\", \"VN\", \"SAS\", \"SP\", \"BhāV\", \"BrS\", \"VibhrV\", \n",
    "                    \"VidhV\", \"PSṬ\", \"HBṬ\", \"NBṬ\",\"PVA\", \"VSṬ\", \"TUS\", \"VyV\", \n",
    "                    \"NM\", \"NyKal\", \"SŚP\", \"ŚVK\", \"HBṬĀ\", \n",
    "                    \"NyKand\", \"AvNir\", \"PVV\", \"TCM\", \"MukV\"\n",
    "                 ]\n",
    "\n",
    "# # others\n",
    "non_priority_texts = [\n",
    "                    \"NBhū\", \n",
    "                    ]\n",
    "\n",
    "# pre-NBhū\n",
    "# priority_texts = [\n",
    "#                     \"VS\", \"MīmS\", \"MMK\", \"ViVy\", \"NS\", \"YSBh\", \"SK\", \n",
    "#                     \"ViṃśV\", \"NBh\", \"MīmBh\", \"ĀP\", \"PSV\", \"NPS\", \"TriṃśBh\", \n",
    "#                     \"YD\", \"PDhS\", \"NV\", \"PPad\", \"ŚV\", \"PVSV\", \"PV\", \"PVin\", \n",
    "#                     \"HB\", \"NB\", \"VN\", \"SAS\", \"SP\", \"BhāV\", \"BrS\", \"VibhrV\", \n",
    "#                     \"VidhV\", \"PSṬ\", \"HBṬ\", \"NBṬ\", \"PVA\", \"VSṬ\", \"TUS\"\n",
    "#                  ]\n",
    "\n",
    "# # others\n",
    "# non_priority_texts = [\n",
    "#                     \"VyV\", \"NM\", \"NyKal\", \"NBhū\", \"SŚP\", \"ŚVK\", \"HBṬĀ\", \n",
    "#                     \"NyKand\", \"AvNir\", \"PVV\", \"TCM\", \"MukV\"\n",
    "#                     ]\n",
    "\n",
    "N_tf_idf_shallow = int( len(doc_ids) * 0.15)\n",
    "N_sw_w_shallow = 200\n",
    "\n",
    "N_tf_idf_deep = int( len(doc_ids) * 1.00)\n",
    "N_sw_w_deep = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for assessing speed and summarizing results \n",
    "\n",
    "from datetime import datetime, date\n",
    "from time import sleep\n",
    "\n",
    "def calc_dur(start, end):\n",
    "    delta = datetime.combine(date.today(), end) - datetime.combine(date.today(), start)\n",
    "    duration_secs = delta.seconds + delta.microseconds / 1000000\n",
    "    return duration_secs\n",
    "\n",
    "def summarize_result(results, label, duration, num_comparisons, display_depth):\n",
    "    print('{} ({:.3f} s, {} comparisons, {:.6f} s/comparison)'.format(\n",
    "            label,\n",
    "            duration, \n",
    "            num_comparisons,\n",
    "            duration/num_comparisons\n",
    "            )\n",
    "    )\n",
    "    for k,v in list(results.items())[:display_depth]:\n",
    "        print(k, \": \", v)\n",
    "    print()\n",
    "    sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlined version of get_closest_docs() with no HTML rendering\n",
    "# two modes: 1) speed_summary gives speed stats for single query; 2) return_values returns full dicts\n",
    "\n",
    "def get_closest_docs_2(query_id, search_depth='shallow', prioritze=True, display_depth=10, mode='speed_summary'):\n",
    "    \n",
    "    if search_depth=='shallow':\n",
    "        N_tf_idf = N_tf_idf_shallow\n",
    "        N_sw_w = N_sw_w_shallow\n",
    "    elif search_depth=='deep':\n",
    "        N_tf_idf = N_tf_idf_deep\n",
    "        N_sw_w = N_sw_w_deep\n",
    "    \n",
    "    # rank candidates by topic similarity\n",
    "\n",
    "    start1 = datetime.now().time()\n",
    "    all_topic_candidates = rank_all_candidates_by_topic_similarity(\n",
    "        query_id\n",
    "        )    \n",
    "    end1 = datetime.now().time()\n",
    "    topic_time = calc_dur(start1, end1)\n",
    "    if mode=='speed_summary': summarize_result(\n",
    "        results=all_topic_candidates,\n",
    "        label='topics',\n",
    "        duration=topic_time,\n",
    "        num_comparisons=len(doc_ids),\n",
    "        display_depth=display_depth\n",
    "    )\n",
    "\n",
    "    if prioritze:\n",
    "        # prioritize candidates by text name, discard secondary candidates\n",
    "            priority_candidate_ids, _ = divide_doc_id_list_by_work_priority(\n",
    "                list(all_topic_candidates.keys()),\n",
    "                priority_texts\n",
    "                )\n",
    "            priority_topic_candidates = { doc_id: all_topic_candidates[doc_id]\n",
    "                for doc_id in priority_candidate_ids\n",
    "                }\n",
    "    else:\n",
    "        # don't prioritize\n",
    "        priority_topic_candidates = all_topic_candidates\n",
    "        priority_topic_candidate_ids = list(all_topic_candidates.keys())\n",
    "    \n",
    "    # limit further computation to only top N_tf_idf of sorted candidates (minus query itself)\n",
    "    pruned_priority_topic_candidates = { k:v\n",
    "        for (k,v) in list(priority_topic_candidates.items())[:N_tf_idf-1]\n",
    "        }\n",
    "\n",
    "    # further rank candidates by tiny tf-idf\n",
    "    start2 = datetime.now().time()\n",
    "    tf_idf_candidates = rank_candidates_by_tiny_TF_IDF_similarity(\n",
    "        query_id,\n",
    "        list(pruned_priority_topic_candidates.keys())\n",
    "        )\n",
    "    end2 = datetime.now().time()\n",
    "    tf_idf_time = calc_dur(start2, end2)\n",
    "    if mode=='speed_summary': summarize_result(\n",
    "        results=tf_idf_candidates,\n",
    "        label='tf-idf',\n",
    "        duration=tf_idf_time,\n",
    "        num_comparisons=N_tf_idf,\n",
    "        display_depth=display_depth\n",
    "    )\n",
    "\n",
    "    # limit further computation to only top N_sw_w of sorted candidates\n",
    "    pruned_tf_idf_candidates = { k:v\n",
    "        for (k,v) in list(tf_idf_candidates.items())[:N_sw_w-1]\n",
    "        }\n",
    "\n",
    "    # further rank candidates by sw_w\n",
    "    start3 = datetime.now().time()\n",
    "    sw_w_candidates = rank_candidates_by_sw_w_alignment_score(\n",
    "        query_id,\n",
    "        list(pruned_tf_idf_candidates.keys())\n",
    "        )\n",
    "    end3 = datetime.now().time()\n",
    "    sw_w_time = calc_dur(start3, end3)\n",
    "\n",
    "    # do not convert sw_w scores of 0.0 to empty string\n",
    "    \n",
    "    # create overall results ordered in terms of sw_w, then tf_idf, then priority topics\n",
    "    overall_results = {}\n",
    "\n",
    "    for k in sw_w_candidates.keys():\n",
    "        overall_results[k] = {'sw_w': sw_w_candidates[k], 'tf_idf': tf_idf_candidates[k], 'priority_topic': priority_topic_candidates[k]}\n",
    "    for k in tf_idf_candidates.keys():\n",
    "        if k not in overall_results:\n",
    "            overall_results[k] = {'sw_w': 0, 'tf_idf': tf_idf_candidates[k], 'priority_topic': priority_topic_candidates[k]}\n",
    "    for k in priority_topic_candidates.keys():\n",
    "        if k not in overall_results:\n",
    "            overall_results[k] = {'sw_w': 0, 'tf_idf': 0, 'priority_topic': priority_topic_candidates[k]}\n",
    "    \n",
    "    if mode=='speed_summary':\n",
    "        summarize_result(\n",
    "            results=sw_w_candidates, \n",
    "            label='sw_w', \n",
    "            duration=sw_w_time, \n",
    "            num_comparisons=N_sw_w, \n",
    "            display_depth=display_depth\n",
    "        )\n",
    "        return\n",
    "\n",
    "    elif mode=='return_values':\n",
    "        return all_topic_candidates, priority_topic_candidates, tf_idf_candidates, sw_w_candidates, overall_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics (0.219 s, 28381 comparisons, 0.000008 s/comparison)\n",
      "PVA_629,vi :  0.9698818351650111\n",
      "PVSV_010,13_010,15 :  0.9523222396152328\n",
      "NBṬ_217,iii_217,v :  0.9382695193578188\n",
      "PVSV_010,19_010,21 :  0.9165924117385076\n",
      "PVin_II,045,iii :  0.913539875583041\n",
      "PVA_612,i_612,iii :  0.9116307949243329\n",
      "PVV_290,iv_290,v :  0.895139987240839\n",
      "PVV_493,viii_493,x :  0.8912334689992081\n",
      "PVin_I,091,i_I,092,i :  0.8910857329884588\n",
      "PVin_II,123,i_II,123,ii :  0.8907492314066945\n",
      "\n",
      "tf-idf (1.327 s, 4257 comparisons, 0.000312 s/comparison)\n",
      "PVin_I,091,i_I,092,i :  0.4476406323861196\n",
      "PVSV_010,13_010,15 :  0.4366356177407236\n",
      "PVV_290,iv_290,v :  0.4250822599417166\n",
      "PVV_291,i_291,iii :  0.35521438566914343\n",
      "PVin_I,092,ii_I,092,iii :  0.3154629129055202\n",
      "PVSV_010,19_010,21 :  0.30638191389324454\n",
      "PVV_292,ii_292,iv :  0.20522585181804726\n",
      "PVV_488,viii_488,x :  0.1716588036028039\n",
      "PSṬ_II,115,ii_II,115,iii :  0.16877487866883265\n",
      "PVSV_011,05_011,12 :  0.15478327326428523\n",
      "\n",
      "sw_w (0.996 s, 200 comparisons, 0.004982 s/comparison)\n",
      "PVin_I,091,i_I,092,i :  93.2\n",
      "PVV_291,i_291,iii :  90.0\n",
      "PVSV_010,19_010,21 :  90.0\n",
      "PVSV_010,13_010,15 :  77.2\n",
      "PVV_290,iv_290,v :  75.4\n",
      "PVin_I,092,ii_I,092,iii :  45.4\n",
      "PVV_394,v_395,i :  28.0\n",
      "PVV_488,viii_488,x :  26.0\n",
      "PVSV_011,05_011,12 :  25.0\n",
      "PVin_I,093,ii :  25.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test single 'speed_summary' run, shallow (no values returned)\n",
    "\n",
    "get_closest_docs_2('NBhū_142,19', search_depth='shallow', display_depth=10, mode='speed_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for 'speed_summary', deep (no values returned)\n",
    "\n",
    "# get_closest_docs_2('NBhū_142,19', search_depth='deep', display_depth=10, mode='speed_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1765 overall, NBhū_104,6^1 – NBhū_154,15 = 140\n",
      "140\n",
      "NBhū_1,7_2,2 NBhū_598,17_598,19\n"
     ]
    }
   ],
   "source": [
    "# identify my NBhū docs\n",
    "\n",
    "NBhu_doc_ids = [ di for di in doc_ids if parse_complex_doc_id(di)[0] == 'NBhū' ]\n",
    "print(len(NBhu_doc_ids), \"overall,\", NBhu_doc_ids[238], \"–\", NBhu_doc_ids[377], '=', 377-238+1)\n",
    "\n",
    "doc_id_full_list = NBhu_doc_ids[238:377+1]\n",
    "print(len(doc_id_full_list))\n",
    "\n",
    "# or do this for all NBhū docs\n",
    "\n",
    "doc_id_full_list = NBhu_doc_ids\n",
    "print(doc_id_full_list[0], doc_id_full_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or take Yuki's PVV docs\n",
    "\n",
    "# doc_ids.index(\"PVV_154,i_154,iii\")\n",
    "# doc_ids.index(\"PVV_176,vi_176,vii\")\n",
    "doc_id_full_list = doc_ids[14265:14340+1]\n",
    "\n",
    "# 14265 = \"PVV_154,i_154,iii\"\n",
    "# 14340 = \"PVV_176,vi_176,vii\"\n",
    "len(doc_id_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized way to choose series of docs within a given work\n",
    "# in interface, there will be one first work drop-down, then two drop-downs for individual doc_ids\n",
    "# do not enforce same text here\n",
    "# do enforce relative ordering here\n",
    "\n",
    "def choose_series(doc_id_1, doc_id_2):\n",
    "\n",
    "    doc_id_1_index = doc_ids.index(doc_id_1)\n",
    "    doc_id_2_index = doc_ids.index(doc_id_2)\n",
    "\n",
    "    # check that doc_id_2 after doc_id_1\n",
    "    if not doc_id_2_index > doc_id_1_index:\n",
    "        print(\"doc_id_2 not after doc_id_1\")\n",
    "        return [doc_id_1] # can always do the first one at least\n",
    "\n",
    "    else:\n",
    "        doc_id_series = [ doc_ids[i] for i in range(doc_id_1_index, doc_id_2_index+1) ]\n",
    "        return doc_id_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "# test for my NBhū docs\n",
    "\n",
    "doc_id_1 = \"NBhū_104,6^1\"\n",
    "doc_id_2 = \"NBhū_154,15\"\n",
    "doc_id_series = choose_series(doc_id_1, doc_id_2)\n",
    "print(len(doc_id_series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1765\n"
     ]
    }
   ],
   "source": [
    "# or all NBhū\n",
    "\n",
    "doc_id_series = choose_series('NBhū_1,7_2,2', 'NBhū_598,17_598,19')\n",
    "print(len(doc_id_series))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "# or for PVV\n",
    "\n",
    "doc_id_series = choose_series('PVV_154,i_154,iii', 'PVV_176,vi_176,vii')\n",
    "print(len(doc_id_series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or for PVA selection\n",
    "doc_id_series = choose_series('PVA_353,ii_353,iv', 'PVA_365,ii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up doc pairs to evaluate as expected from benchmark\n",
    "\n",
    "doc_id_suspected_pair_list = [\n",
    "    ('NBhū_104,6^1', 'PVin_I,034,i'),\n",
    "    ('NBhū_104,6^2', 'PV_3.148ab_3.150cd'),\n",
    "    ('NBhū_104,6^2', 'NS_4.2.8_4.2.14'),\n",
    "    ('NBhū_106,3', 'NS_4.2.23_4.2.28'),\n",
    "    ('NBhū_106,3', 'NV_487,02_487,04'),\n",
    "    ('NBhū_106,11_107,1', 'ViṃśV_93,i_95,i'),\n",
    "    ('NBhū_106,11_107,1', 'PVin_I,035,i_I,036,ii'),\n",
    "    ('NBhū_106,11_107,1', 'NS_4.2.15_4.2.22'),\n",
    "    ('NBhū_107,6_108,1', 'PVin_I,039,i_I,039,ii'),\n",
    "    ('NBhū_108,4_108,6', 'PVin_I,040,i'),\n",
    "    ('NBhū_108,10', 'PVin_I,041,i'),\n",
    "    ('NBhū_108,10', 'PV_3.431cd_3.434cd'),\n",
    "    ('NBhū_109,1', 'PV_3.431cd_3.434cd'),\n",
    "    ('NBhū_109,7', 'PV_3.329ab_3.332ab'),\n",
    "    ('NBhū_109,7', 'PVin_I,035,i_I,036,ii'),\n",
    "    ('NBhū_115,1_115,4', 'PV_3.281ab_3.284ab'),\n",
    "    ('NBhū_115,18', 'PVA_289,xiv_290,ii'),\n",
    "    ('NBhū_115,18', 'PVA_290,iii'),\n",
    "    ('NBhū_115,18', 'PVA_290,iv_290,vi'),\n",
    "    ('NBhū_116,7', 'NS_4.1.35_4.1.42'),\n",
    "    ('NBhū_116,7', 'NV_454,17_454,18'),\n",
    "    ('NBhū_117,3^1', 'PVA_288,vii'),\n",
    "    ('NBhū_117,3^2', 'PVA_288,vii'),\n",
    "    ('NBhū_117,3^2', 'PV_3.208cd_3.211ab'),\n",
    "    ('NBhū_121,2^2', 'PVin_I,046,i_I,046,iii'),\n",
    "    ('NBhū_124,8^2', 'PV_2.066ab_2.069ab'),\n",
    "    ('NBhū_125,15', 'ŚV_5,4.250ab_5,4.253ab'),\n",
    "    ('NBhū_126,6^1', 'NS_4.2.8_4.2.14'),\n",
    "    ('NBhū_126,6^1', 'NBh_1047,i_1047,ii'),\n",
    "    ('NBhū_126,6^1', 'NS_2.1.33_2.1.39'),\n",
    "    ('NBhū_126,6^3', 'NS_2.1.33_2.1.39'),\n",
    "    ('NBhū_126,6^3', 'NS_4.2.8_4.2.14'),\n",
    "    ('NBhū_131,11_131,17', 'ViṃśV_93,i_95,i'),\n",
    "    ('NBhū_132,2', 'NS_4.2.15_4.2.22'),\n",
    "    ('NBhū_132,11^1', 'TUS_ii,102,i_ii,102,ii'),\n",
    "    ('NBhū_132,11^1', 'PV_3.386cd_3.389cd'),\n",
    "    ('NBhū_138,9', 'PVA_353,x'),\n",
    "    ('NBhū_138,9', 'PVA_353,xi_353,xii'),\n",
    "    ('NBhū_139,1_139,3', 'PVA_353,xiii_353,xv'),\n",
    "    ('NBhū_139,1_139,3', 'PVSV_022,06_022,20'),\n",
    "    ('NBhū_139,1_139,3', 'PVin_I,086,ii^1'),\n",
    "    ('NBhū_139,26_140,1', 'PV_3.326ab_3.328cd'),\n",
    "    ('NBhū_140,21', 'PVin_I,046,i_I,046,iii'),\n",
    "    ('NBhū_142,2', 'PV_3.329ab_3.332ab'),\n",
    "    ('NBhū_142,12', 'PVA_387,xvii_387,xxii'),\n",
    "    ('NBhū_142,12', 'PVA_359,iv_359,vi'),\n",
    "    ('NBhū_142,19', 'PVA_361,xiii_361,xvi'),\n",
    "    ('NBhū_142,19', 'PVSV_010,13_010,15'),\n",
    "    ('NBhū_142,19', 'PVSV_010,19_010,21'),\n",
    "    ('NBhū_142,19', 'PVin_I,091,i_I,092,i'),\n",
    "    ('NBhū_142,19', 'PVin_I,092,ii_I,092,iii'),\n",
    "    ('NBhū_144,20^1', 'PVA_361,xiii_361,xvi'),\n",
    "    ('NBhū_145,15', 'PVA_360,ix'),\n",
    "    ('NBhū_145,15', 'PVA_360,x'),\n",
    "    ('NBhū_145,22', 'PVA_360,x'),\n",
    "    ('NBhū_145,22', 'PVA_360,xi_361,i'),\n",
    "    ('NBhū_146,7', 'PVA_360,xi_361,i'),\n",
    "    ('NBhū_146,7', 'PVA_361,ii_361,iii'),\n",
    "    ('NBhū_146,14_146,18', 'PVA_361,ii_361,iii'),\n",
    "    ('NBhū_146,14_146,18', 'PVA_361,iv_361,vi'),\n",
    "    ('NBhū_146,21', 'PVA_361,iv_361,vi'),\n",
    "    ('NBhū_146,21', 'PVA_361,vii'),\n",
    "    ('NBhū_147,3_147,6', 'PVA_361,x_361,xii'),\n",
    "    ('NBhū_149,4_149,16', 'PVA_366,v_366,ix'),\n",
    "    ('NBhū_149,19', 'PVA_366,iv'),\n",
    "    ('NBhū_150,1', 'HB_3,1^1'),\n",
    "    ('NBhū_150,1', 'PV_2.001ab_2.005cd'),\n",
    "    ('NBhū_150,6^2', 'PVin_II,001,i_II,001,ii'),\n",
    "    ('NBhū_150,6^2', 'NB_3.1_3.8'),\n",
    "    ('NBhū_153,4_153,7', 'PVA_356,iv_356,vii'),\n",
    "    ('NBhū_153,14', 'PVA_358,ix^4')\n",
    "]\n",
    "doc_id_suspected_pair_list = []\n",
    "\n",
    "num_expected_pairs = len(doc_id_suspected_pair_list)\n",
    "\n",
    "# turn into dict of lists\n",
    "doc_id_suspected_pair_dict = {}\n",
    "for doc_id_1, doc_id_2 in doc_id_suspected_pair_list:\n",
    "    if doc_id_1 not in doc_id_suspected_pair_dict:\n",
    "        doc_id_suspected_pair_dict[doc_id_1] = [doc_id_2]\n",
    "    else:\n",
    "        doc_id_suspected_pair_dict[doc_id_1].append(doc_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for describing doc_id's position in ranked results as well as giving absolute score\n",
    "\n",
    "def format_score_summary(ranking_dict, doc_id):\n",
    "\n",
    "    if doc_id in ranking_dict:\n",
    "\n",
    "        ks = list(ranking_dict.keys())\n",
    "        rank = ks.index(doc_id) + 1\n",
    "        score = ranking_dict[doc_id]\n",
    "\n",
    "        return \"{} ({:.2f})\".format(rank, score)\n",
    "\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16886a5cf24f45f3839c4f7a93be6b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=76.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PVV_154,i_154,iii\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6b4855b9787f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# mode name a bit obsolete, most important is that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     all_topic_candidates, priority_topic_candidates, tf_idf_candidates, sw_w_candidates = get_closest_docs_2(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdoc_id_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'shallow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'return_values'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# compare performance against benchmark\n",
    "# i.e., loop over get_closest_docs_2() in mode='return_values'\n",
    "# outputs to tsv spreadsheet\n",
    "\n",
    "pbar = tqdm(total=len(doc_id_full_list))\n",
    "\n",
    "output_buffer_1 = \"\"\n",
    "output_buffer_2 = '\\t'.join([\n",
    "    \"doc_id\", \"num_expected_pairs\", \"num_pairs_confirmed\",\n",
    "    \"num_novel_pairs[50]\", \"num_novel_pairs[40]\", \"num_novel_pairs[30]\"\n",
    "    ]) + '\\n'\n",
    "i = 0\n",
    "rank_threshold = 5 # this determines whether system \"CONFIRMS\" an expected pair\n",
    "sw_w_min_threshold = 50 # this determines a \"NOVEL PAIR\"\n",
    "sw_w_threshold_for_display = 50\n",
    "for doc_id_1 in doc_id_full_list:\n",
    "\n",
    "    # announce new doc_id_1\n",
    "    \n",
    "    print()\n",
    "    print(doc_id_1)\n",
    "    something_to_say = False # in case totally uneventful, will output \"(none)\"\n",
    "\n",
    "    # perform full search of doc_id_1 with get_closest_docs_2() in 'return_values' mode\n",
    "    # mode name a bit obsolete, most important is that \n",
    "\n",
    "    all_topic_candidates, priority_topic_candidates, tf_idf_candidates, sw_w_candidates = get_closest_docs_2(\n",
    "        doc_id_1, search_depth='shallow', mode='return_values'\n",
    "    )\n",
    "\n",
    "    # get doc_id_2 pair suspects\n",
    "\n",
    "    if doc_id_1 in doc_id_suspected_pair_dict:\n",
    "        doc_id_2_list = doc_id_suspected_pair_dict[doc_id_1]\n",
    "        num_pairs_confirmed = 0\n",
    "    else:\n",
    "        doc_id_2_list = []\n",
    "        num_pairs_confirmed = \"\"\n",
    "\n",
    "    # evaluate each supposed pair\n",
    "    \n",
    "    num_expected_pairs = len(doc_id_2_list)\n",
    "    for doc_id_2 in doc_id_2_list[:0]:\n",
    "        \n",
    "        something_to_say = True\n",
    "\n",
    "        # format four ranked scores as \"rank (score)\"\n",
    "        topic_all_scores = format_score_summary(all_topic_candidates, doc_id_2)\n",
    "        topic_priority_scores = format_score_summary(priority_topic_candidates, doc_id_2)\n",
    "        tf_idf_scores = format_score_summary(tf_idf_candidates, doc_id_2)\n",
    "        sw_w_scores = format_score_summary(sw_w_candidates, doc_id_2)\n",
    "\n",
    "        # format results for output to spreadsheet\n",
    "        output_buffer_1 += '\\t'.join([\n",
    "            str(i),\n",
    "            doc_id_1,\n",
    "            doc_id_2,\n",
    "            topic_all_scores,\n",
    "            topic_priority_scores,\n",
    "            tf_idf_scores,\n",
    "            sw_w_scores,\n",
    "        ]) + '\\n'\n",
    "        \n",
    "        # have msg ready to confirm in notebook significant scores for suspected pairs\n",
    "        if sw_w_scores == \"\":\n",
    "            rank = 0\n",
    "            sw_w_abs_score = 0\n",
    "        else:\n",
    "            rank_str, sw_w_abs_score_str = sw_w_scores.split(' ', 1)\n",
    "            rank = int(rank_str)\n",
    "            sw_w_abs_score = float(sw_w_abs_score_str[1:-1])\n",
    "                \n",
    "        confirmation_msg = (0 < rank <= rank_threshold) * '(CONFIRMED)'\n",
    "        num_pairs_confirmed += bool(confirmation_msg) * 1\n",
    "\n",
    "        # output to notebook\n",
    "        i += 1\n",
    "        print(\"pair #{}/{} ({}, {}): {} {} {} {} {}\".format(\n",
    "            i, num_expected_pairs,\n",
    "            doc_id_1, doc_id_2,\n",
    "            topic_all_scores,\n",
    "            topic_priority_scores,\n",
    "            tf_idf_scores,\n",
    "            sw_w_scores,\n",
    "            confirmation_msg\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # also report novel findings\n",
    "        \n",
    "    num_novel_pairs = {30: 0, 40: 0, 50: 0}\n",
    "    for k,v in sw_w_candidates.items():\n",
    "\n",
    "        if v > sw_w_min_threshold and k not in doc_id_2_list:\n",
    "            \n",
    "            something_to_say = True\n",
    "\n",
    "            for threshold in num_novel_pairs.keys():\n",
    "                if v > threshold:\n",
    "                    num_novel_pairs[threshold] += 1\n",
    "\n",
    "            # format four ranked scores as \"rank (score)\"\n",
    "            topic_all_scores = format_score_summary(all_topic_candidates, k)\n",
    "            topic_priority_scores = format_score_summary(priority_topic_candidates, k)\n",
    "            tf_idf_scores = format_score_summary(tf_idf_candidates, k)\n",
    "            sw_w_scores = format_score_summary(sw_w_candidates, k)\n",
    "\n",
    "            # format results for output to spreadsheet\n",
    "            output_buffer_1 += '\\t'.join([\n",
    "                \"no id\",\n",
    "                doc_id_1,\n",
    "                k,\n",
    "                topic_all_scores,\n",
    "                topic_priority_scores,\n",
    "                tf_idf_scores,\n",
    "                sw_w_scores,\n",
    "            ]) + '\\n'\n",
    "\n",
    "            # output to notebook\n",
    "            print(\"NOVEL PAIR ({}, {}): {} {} {} {}\".format(\n",
    "                doc_id_1, k,\n",
    "                topic_all_scores,\n",
    "                topic_priority_scores,\n",
    "                tf_idf_scores,\n",
    "                sw_w_scores,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # stop once scores too low\n",
    "        elif (v < sw_w_min_threshold):\n",
    "            break\n",
    "    \n",
    "    if num_expected_pairs > 0:\n",
    "        print(\"expected pairs confirmed (@{}): {}/{}\".format(rank_threshold, num_pairs_confirmed, num_expected_pairs))\n",
    "    if num_novel_pairs[sw_w_threshold_for_display] > 0:\n",
    "        print(\"novel pairs (@{}): {}\".format(sw_w_min_threshold, num_novel_pairs[sw_w_threshold_for_display]))\n",
    "        \n",
    "    # give explicit negative output in notebook if there are neither suspected pairs nor novel findings\n",
    "    if not something_to_say:\n",
    "        print(\"(none)\")\n",
    "\n",
    "    # prepare final summary of doc_1 output to second file\n",
    "    output_buffer_2 += \"{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "        doc_id_1, num_expected_pairs, num_pairs_confirmed,\n",
    "        num_novel_pairs[50], num_novel_pairs[40], num_novel_pairs[30]\n",
    "        ) + '\\n'\n",
    "        \n",
    "    # update progress bar as last thing\n",
    "    pbar.update()        \n",
    "\n",
    "            \n",
    "# finish up\n",
    "\n",
    "with open('pairs.tsv','w') as f_out_1:\n",
    "    f_out_1.write(output_buffer_1)\n",
    "\n",
    "with open('doc_1_summaries.tsv','w') as f_out_2:\n",
    "    f_out_2.write(output_buffer_2)\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(overall_results[k]['sw_w'] >= 50) | False | False\n"
     ]
    }
   ],
   "source": [
    "# define complex threshold condition\n",
    "# can specify 1–3 options (\"1\",\"2\",\"3\" below) (\"OR\")\n",
    "# within each option, can specify lower bounds for 1–3 measures ('sw_w', 'tf_idf', 'priority_topic') (\"AND\")\n",
    "# cannot yet specify upper bounds\n",
    "\n",
    "threshold_inputs = {\n",
    "    1: {\n",
    "        'sw_w':\"50\",\n",
    "        'tf_idf':\"\",\n",
    "        'priority_topic':\"\"\n",
    "        },\n",
    "    2: {\n",
    "        'sw_w':\"\",\n",
    "        'tf_idf':\"\",\n",
    "        'priority_topic':\"\"\n",
    "        },\n",
    "    3: {\n",
    "        'sw_w':\"\",\n",
    "        'tf_idf':\"\",\n",
    "        'priority_topic':\"\"\n",
    "        },\n",
    "}\n",
    "\n",
    "conditions = {} # int to string\n",
    "\n",
    "for i in range(1,4):\n",
    "\n",
    "    # create individual \"AND\" expression\n",
    "\n",
    "    conditions[i] = \"\"\n",
    "\n",
    "    for measure in ['sw_w', 'tf_idf', 'priority_topic']:\n",
    "        if threshold_inputs[i][measure] == \"\":\n",
    "            pass\n",
    "        else:\n",
    "            if conditions[i] != \"\":\n",
    "                conditions[i] += \" & \"\n",
    "            conditions[i] += \"(overall_results[k]['{}'] >= {})\".format(\n",
    "                measure, \n",
    "                threshold_inputs[i][measure]\n",
    "            )\n",
    "\n",
    "    if conditions[i] == \"\":\n",
    "        conditions[i] = \"False\" # empty condition just contributes False to OR\n",
    "\n",
    "# create overall \"OR\" expression\n",
    "overall_condition = \"{} | {} | {}\".format(conditions[1], conditions[2], conditions[3])\n",
    "\n",
    "print(overall_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:11:50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "T_topic  = 0.000007\n",
    "T_tf_idf = 0.000315\n",
    "T_sw_w   = 0.004513\n",
    "query_time_estimate = T_topic * len(doc_ids) + T_tf_idf * N_tf_idf_shallow + T_sw_w * N_sw_w_shallow\n",
    "total_time_estimate = query_time_estimate * len(doc_id_series)\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(total_time_estimate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8269e3751c4e8ca403a86ea805aa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "need to streamline above presentations into one main output format\n",
    "    HTML table to PDF seems like good default\n",
    "\n",
    "it will NOT include any of the \"confirm\" functionality assuming \"expected pairs\"\n",
    "\n",
    "rather, it will be based on one or more thresholds\n",
    "    sw_w is clear and simplest, 30–50 are reasonable values\n",
    "    but could also set thresholds for tf_idf and topic (both [0,1])\n",
    "    threshold comparisons can be combined either as \"AND\" (i.e., all required) or as \"OR\" (i.e., any suffices)\n",
    "    actually most desirable is a series of nested conditions\n",
    "        e.g. (sw_w > 50) OR (sw_w > 30 AND tf_idf > 0.8) OR (topic > 0.9 AND tf_idf > 0.5)\n",
    "\"\"\"\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "pbar = tqdm(total=len(doc_id_series))\n",
    "\n",
    "# title\n",
    "HTML_buffer = \"\"\"\n",
    "<h1 align=\"center\">Search Results for {}</h1>\"\"\".format(\n",
    "    '{} – {}'.format(doc_id_series[0], doc_id_series[-1])\n",
    "    )\n",
    "\n",
    "# start table with header row\n",
    "HTML_buffer += \"\"\"\n",
    "<table border=\"1px solid #dddddd;\" width=\"100%\">\n",
    "  <thead>\n",
    "    <tr align=\"center\">\n",
    "      <th width=\"10%\">{}</th>\n",
    "      <th width=\"10%\">{}</th>\n",
    "      <th width=\"5%\">{}</th>\n",
    "      <th width=\"5%\">{}</th>\n",
    "      <th width=\"5%\">{}</th>\n",
    "      <th width=\"5%\">{}</th>\n",
    "      <th width=\"25%\">{}</th>\n",
    "      <th width=\"5%\">{}</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "\"\"\".format( 'doc_id_1',\n",
    "            'doc_id_2',\n",
    "            'topic (all)',\n",
    "            'topic (priority)',\n",
    "            'tf-idf',\n",
    "            'sw',\n",
    "            'text of best match (doc 1)',\n",
    "            'dcCp url'\n",
    "            )\n",
    "\n",
    "# added and subtracted throughout search for continuously refreshable results\n",
    "HTML_buffer_suffix = \"\"\"\n",
    "  </tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "for i, doc_id_1 in enumerate(doc_id_series):\n",
    "\n",
    "    # perform full search of doc_id_1 with get_closest_docs_2() in 'return_values' mode\n",
    "    all_topic_candidates, priority_topic_candidates, tf_idf_candidates, sw_w_candidates, overall_results = get_closest_docs_2(\n",
    "        doc_id_1, search_depth='shallow', mode='return_values'\n",
    "    )\n",
    "    \n",
    "    for k in overall_results.keys():\n",
    "\n",
    "        if eval(overall_condition):\n",
    "\n",
    "            # format four ranked scores as \"rank (score)\"\n",
    "            topic_all_scores = format_score_summary(all_topic_candidates, k)\n",
    "            topic_priority_scores = format_score_summary(priority_topic_candidates, k)\n",
    "            tf_idf_scores = format_score_summary(tf_idf_candidates, k)\n",
    "            sw_w_scores = format_score_summary(sw_w_candidates, k)\n",
    "\n",
    "            # do alignment again, not super efficient but ok for now, to avoid complicating rank_by_sw\n",
    "            text_1 = doc_fulltext[doc_id_1]\n",
    "            text_2 = doc_fulltext[k]\n",
    "            subseq1_pos, subseq2_pos, subseq1_len, subseq2_len, score = sw_align(text_1, text_2, words=False)\n",
    "            subseq_in_text_1 = text_1[subseq1_pos:subseq1_pos+subseq1_len]\n",
    "\n",
    "            url = \"https://www.vatayana.info/docCompare?doc_id_1={}&doc_id_2={}\".format(\n",
    "                urllib.parse.quote(doc_id_1), urllib.parse.quote(k)\n",
    "                )\n",
    "            link = \"<a href='{}' target='vatayana'>dcCp ↪</a>\".format(url)\n",
    "\n",
    "            # format results for output\n",
    "            HTML_buffer += \"\"\"\n",
    "                <tr align=\"center\">\n",
    "                  <td>{}</td>\n",
    "                  <td>{}</td>\n",
    "                  <td>{}</td>\n",
    "                  <td>{}</td>\n",
    "                  <td>{}</td>\n",
    "                  <td>{}</td>\n",
    "                  <td align=\"left\">{}</td>\n",
    "                  <td>{}</td>\n",
    "                </tr>\n",
    "            \"\"\".format(\n",
    "                doc_id_1,\n",
    "                k,\n",
    "                topic_all_scores,\n",
    "                topic_priority_scores,\n",
    "                tf_idf_scores,\n",
    "                sw_w_scores,\n",
    "                subseq_in_text_1,\n",
    "                link\n",
    "                )\n",
    "    \n",
    "    HTML_buffer += HTML_buffer_suffix\n",
    "    \n",
    "    time_left = total_time_estimate * (len(doc_id_series)-i-1)/len(doc_id_series)\n",
    "    fudge_factor = 2 # old estimates are off! need to better understand and improve\n",
    "    time_left *= fudge_factor \n",
    "    progress_update = \"<h2 align='center'>In progress... {}/{} complete, approximately {} remaining</h2>\".format(\n",
    "        i+1, \n",
    "        len(doc_id_series),\n",
    "        time.strftime('%H:%M:%S', time.gmtime(time_left))\n",
    "    )\n",
    "    HTML_buffer += progress_update\n",
    "    \n",
    "    with open('similarity_results.html','w') as f_out:\n",
    "        f_out.write(HTML_buffer)\n",
    "    \n",
    "    HTML_buffer = HTML_buffer[:-len(progress_update)] # i.e., HTML_buffer -= progress_update\n",
    "    HTML_buffer = HTML_buffer[:-len(HTML_buffer_suffix)] # i.e., HTML_buffer -= HTML_buffer_suffix\n",
    "    \n",
    "    pbar.update()        \n",
    "\n",
    "# finish up\n",
    "\n",
    "HTML_buffer += HTML_buffer_suffix\n",
    "\n",
    "progress_update = \"<h2 align='center'>Completed {}/{} queries.</h2>\".format(\n",
    "        i+1,\n",
    "        len(doc_id_series)\n",
    "    )\n",
    "HTML_buffer += progress_update\n",
    "    \n",
    "with open('similarity_results.html','w') as f_out:\n",
    "    f_out.write(HTML_buffer)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
